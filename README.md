# scraping依頼
このrepogitoryは研究に使用するデータをスクレイピングを用いて取得する際の依頼に使用します。
必要な環境やコード管理にも使用するのでscrapyに見識がある人は環境を整えれば自身で実行することも可能です。

# スクレイピングタスク依頼フォーマット

このフォーマットを使用して、Scrapyを利用して特定のウェブサイトからデータをスクレイピングするタスクを依頼できます。
以下のフォーマットを参考にissueに依頼をあげてください。

## タスクの詳細

- **対象のウェブサイトのURL**: ここにスクレイピングしたいウェブサイトのURLを記述してください。

- **取得したいデータ項目**: どのデータ項目をスクレイピングしたいか、具体的に指定してください。例えば、タイトル、価格、説明文、画像URLなどのデータ項目をリストアップしてください。

- **データの保存方法**: スクレイピングしたデータをどの形式で保存するか（CSV、JSON、データベースなど）を指定してください。

- **スクレイピングの頻度**: 一度だけスクレイピングするのか、定期的に更新する必要があるのか、頻度について指定してください。

## その他の要件

- **ログインが必要か**: ターゲットウェブサイトにログインが必要な場合、アカウント情報やログイン手順について提供してください。

- **スクレイピングの制約**: ウェブサイトにアクセスする際のクローラーの動作に関する特別な要件や制約がある場合、それについて詳細を提供してください。

## 期待される成果物

- スクレイピングされたデータを保存するためのファイルまたはデータベース。

- スクレイピングスクリプトのコード。

## その他のコメントや特別な要望

ここに追加のコメント、特別な要望、またはプロジェクトに関連するその他の情報を提供してください。

# 実行方法
まずはこのrepositoryをcloneしてください
## scrapyを既に使用できる環境を持っている場合
```
cd scraping_request/scraping_requests/scraping_requests/spiders
scrapy crawl {spider file} -o {output_file_name}.{自分の希望の拡張子(csv,json etc...)}
```

## scrapyの環境がない場合
今回の場合pipenv使用し環境を作成できるようにしています。
```
cd scraping_request
pip install pipenv
pipenv sync
pipenv shell
cd scraping_requests/scraping_requests/spiders
scrapy crawl {spider file} -o {output_file_name}.{希望の拡張子}
```
